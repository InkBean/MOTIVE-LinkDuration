{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Armin. Slightly modified by me. \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split # splitting the data\n",
    "from sklearn.preprocessing import StandardScaler # normalization\n",
    "from sklearn.ensemble import RandomForestRegressor # feature importance\n",
    "from sklearn.experimental import enable_iterative_imputer # filling nan\n",
    "from sklearn.impute import IterativeImputer # filling nan\n",
    "from scipy import stats # finding outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the path of the parent directory to preprocess all of the csv files at once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/JunxiChen/Documents/data\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "PATH = (os.path.abspath(os.path.join(PATH, os.pardir)))\n",
    "PATH += \"/data\"\n",
    "print(PATH)\n",
    "\n",
    "HEADINGS = ['ID_X', 'ID_Y', 'localx_X', 'localy_X', 'globalx_X', 'globaly_X', 'vlength_X','vwidth_X', 'vclass_X', 'vel_X',\n",
    "            'acc_X', 'laneID_X', 'ozone_X', 'dzone_X', 'int_X', 'section_X', 'direction_X', 'movement_X', 'preceeding_X',\n",
    "            'following_X', 'space_headway_X', 'time_headway_X', 'location_X', \n",
    "            'localx_Y', 'localy_Y', 'globalx_Y', 'globaly_Y', 'vlength_Y','vwidth_Y', 'vclass_Y', 'vel_Y',\n",
    "            'acc_Y', 'laneID_Y', 'ozone_Y', 'dzone_Y', 'int_Y', 'section_Y', 'direction_Y', 'movement_Y', 'preceeding_Y',\n",
    "            'following_Y', 'space_headway_Y', 'time_headway_Y', 'location_Y', 'duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define some functions that will help with preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get X and Y values from the dataFrames\n",
    "def getXandY(dataFrame):\n",
    "    dataFrame_np = dataFrame.values()\n",
    "    train, test = train_test_split(dataFrame_np, test_size=0.2)\n",
    "    yPosition_train = train.shape[1] - 1\n",
    "    trainX = train[:, :-1].values\n",
    "    trainY = train[:, yPosition_train]\n",
    "\n",
    "    yPosition_test = test.shape[1] - 1\n",
    "    testX = test[:, :-1].values\n",
    "    testY = test[:, yPosition_test]\n",
    "    return [trainX, trainY] , [testX, testY]\n",
    "\n",
    "\n",
    "# function that will Normalize the data using Standard Scaling\n",
    "def normalize(data):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# function that will fill in the NaN values using MICE imputation method\n",
    "def mvImp(df_incomplete):\n",
    "    # fill the na values with 9999\n",
    "    df_incomplete = df_incomplete.fillna(9999)\n",
    "    # create an object of type IterativeImputer from the sklearn class\n",
    "    imputer = IterativeImputer(missing_values = 9999, max_iter=30, random_state=0,\n",
    "                 add_indicator=False, estimator=None,\n",
    "                 imputation_order='ascending', initial_strategy='mean', \n",
    "                 max_value=None, min_value=None, \n",
    "                 n_nearest_features=None,\n",
    "                 tol=0.001, verbose=0)\n",
    "    # fit the object on our incomplete data\n",
    "    imputer.fit(df_incomplete)\n",
    "    # transform the data and store into a new variable\n",
    "    imputedData = imputer.transform(df_incomplete)\n",
    "\n",
    "    return imputedData\n",
    "\n",
    "# function that will remove outliers\n",
    "def removeOutliers(data):\n",
    "    z = np.abs(stats.zscore(data))\n",
    "    threshold = 3\n",
    "    data = data[(z < 3).all(axis=1)]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# function that will save the npArray into either the train or test directory and if it is X or Y\n",
    "def saveNumpy(npArray, train_or_test, XorY):\n",
    "    global PATH\n",
    "    np.savetxt(PATH+'/'+train_or_test+'/'+XorY+'.csv', npArray, delimiter=',')\n",
    "\n",
    "# function that converts the location to a one hot encoded vector\n",
    "def convert_location(dataFrame):\n",
    "    # dictionary of names converted to integers to map quicker\n",
    "    location_names = {'lankershim' : 0.0, 'us-101' : 1.0, 'peachtree': 2.0, 'i-80': 3.0}\n",
    "    \n",
    "    dataFrame = dataFrame.replace(location_names)\n",
    "    return dataFrame\n",
    "\n",
    "# function that writes the feature importance to a csv file for analysis (dropping location for now)\n",
    "def rf_feature_importance(dataFrame, fileName):\n",
    "    global PATH\n",
    "    file = (PATH + ' ' + fileName + '.txt', 'w')\n",
    "    \n",
    "    dataFrame = dataFrame.drop(['ID_X','ID_Y','ozone_X','ozone_Y','preceeding_X','preceeding_Y','following_X','following_Y',\n",
    "                                'space_headway_Y','space_headway_X','time_headway_X', 'time_headway_Y', 'localx_Y'], axis=1)\n",
    "    \n",
    "    names = dataFrame.columns\n",
    "    Y = dataFrame.pop('duration')\n",
    "    X = dataFrame\n",
    "    \n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X, Y)\n",
    "    \n",
    "    print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), names), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add headings to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peach_50m.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# returns a list of all files with specified ending\n",
    "all_files = glob.glob(PATH + '/*.csv')\n",
    "i = 0\n",
    "data_frame_holder = {}\n",
    "\n",
    "for filename in all_files:\n",
    "    print(os.path.basename(filename))\n",
    "    \n",
    "    key = os.path.basename(filename)\n",
    "    value = pd.read_csv(filename, names = HEADINGS)\n",
    "    data_frame_holder.update({key:value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Convert the location 2) remove outliers 3) normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID_X', 'ID_Y', 'localx_X', 'localy_X', 'globalx_X', 'globaly_X',\n",
      "       'vlength_X', 'vwidth_X', 'vclass_X', 'vel_X', 'acc_X', 'laneID_X',\n",
      "       'ozone_X', 'dzone_X', 'int_X', 'section_X', 'direction_X', 'movement_X',\n",
      "       'preceeding_X', 'following_X', 'space_headway_X', 'time_headway_X',\n",
      "       'location_X', 'localx_Y', 'localy_Y', 'globalx_Y', 'globaly_Y',\n",
      "       'vlength_Y', 'vwidth_Y', 'vclass_Y', 'vel_Y', 'acc_Y', 'laneID_Y',\n",
      "       'ozone_Y', 'dzone_Y', 'int_Y', 'section_Y', 'direction_Y', 'movement_Y',\n",
      "       'preceeding_Y', 'following_Y', 'space_headway_Y', 'time_headway_Y',\n",
      "       'location_Y', 'duration'],\n",
      "      dtype='object')\n",
      "(10791, 45)\n"
     ]
    }
   ],
   "source": [
    "for name, dataFrame in data_frame_holder.items():\n",
    "    print(dataFrame.columns)\n",
    "    dataFrame = convert_location(dataFrame)\n",
    "    dataFrame = mvImp(dataFrame)\n",
    "    dataFrame = normalize(dataFrame)\n",
    "    data_frame_holder.update({name : dataFrame})\n",
    "    print(dataFrame.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I would like to see what values account for the most variance by using a random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID_X', 'ID_Y', 'localx_X', 'localy_X', 'globalx_X', 'globaly_X',\n",
      "       'vlength_X', 'vwidth_X', 'vclass_X', 'vel_X', 'acc_X', 'laneID_X',\n",
      "       'ozone_X', 'dzone_X', 'int_X', 'section_X', 'direction_X', 'movement_X',\n",
      "       'preceeding_X', 'following_X', 'space_headway_X', 'time_headway_X',\n",
      "       'location_X', 'localx_Y', 'localy_Y', 'globalx_Y', 'globaly_Y',\n",
      "       'vlength_Y', 'vwidth_Y', 'vclass_Y', 'vel_Y', 'acc_Y', 'laneID_Y',\n",
      "       'ozone_Y', 'dzone_Y', 'int_Y', 'section_Y', 'direction_Y', 'movement_Y',\n",
      "       'preceeding_Y', 'following_Y', 'space_headway_Y', 'time_headway_Y',\n",
      "       'location_Y', 'duration'],\n",
      "      dtype='object')\n",
      "[(0.4583, 'vel_Y'), (0.2222, 'acc_Y'), (0.125, 'dzone_X'), (0.1111, 'localy_Y'), (0.0833, 'vel_X'), (0.0, 'vwidth_Y'), (0.0, 'vwidth_X'), (0.0, 'vlength_Y'), (0.0, 'vlength_X'), (0.0, 'vclass_Y'), (0.0, 'vclass_X'), (0.0, 'section_Y'), (0.0, 'section_X'), (0.0, 'movement_Y'), (0.0, 'movement_X'), (0.0, 'location_Y'), (0.0, 'location_X'), (0.0, 'localy_X'), (0.0, 'localx_X'), (0.0, 'laneID_Y'), (0.0, 'laneID_X'), (0.0, 'int_Y'), (0.0, 'int_X'), (0.0, 'globaly_Y'), (0.0, 'globaly_X'), (0.0, 'globalx_Y'), (0.0, 'globalx_X'), (0.0, 'dzone_Y'), (0.0, 'direction_Y'), (0.0, 'direction_X'), (0.0, 'acc_X')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abaz/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for fileName, dataFrame in data_frame_holder.items():\n",
    "    holder = pd.DataFrame(dataFrame)\n",
    "    holder.columns = HEADINGS\n",
    "    print(holder.columns)\n",
    "    rf_feature_importance(holder, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
